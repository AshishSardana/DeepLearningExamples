# Docker image to be used by tritonserver
triton_docker_image: gitlab-master.nvidia.com:5005/dl/dgx/tritonserver/tritonserver-21.02-modified:cublas_11.2_cudnn_8.0.4_tf_2.3.0-devel

# Path to the Model Repository
model_repository: /workspace/examples/inference/models

# How Model Analyzer will launch triton. It should
# be either "docker", "local", or "remote".
# See docs/launch_modes.md for more information.
triton_launch_mode: docker

# List of the model names to be analyzed
model_names:
  widedeep:
    model_config_parameters:
     dynamic_batching:
      max_queue_delay_microseconds: 0
     max_batch_size: 256
     instance_group:
      -
        kind: KIND_GPU
        count: [1, 2]

# Concurrency values to be used for the analysis
concurrency: [256, 320, 384, 448, 512, 576, 640, 768, 896, 1024, 1152, 1280]
#   start: 1
#   stop: 16
#   step: 1

# Batch size values to be used for the analysis
batch_sizes: 1

# Model config values to use for the sweep
model_config_parameters:
   dynamic_batching:
      max_queue_delay_microseconds: 0
   max_batch_size: 256
   instance_group:
   -
      kind: KIND_GPU
      count: [1, 2]

# Whether to export metrics to a file
export: true

# Export path to be used
export_path: "/workspace/examples/results/model_analyzer/widedeep/docker/ic-12_cc-varied_bs-1_db-10ms"

# File name to be used for the model inference results

# File name to be used for the GPU metrics results

# File name to be used for storing the server only metrics.

# Specifies the maximum number of retries for any retry attempt.

# Specifies how long (seconds) to gather server-only metrics
duration_seconds: 10

# Duration of waiting time between each metric measurement in seconds

# The protocol used to communicate with the Triton Inference Server. Only 'http' and 'grpc' are allowed for the values.
client_protocol: grpc

# The full path to the perf_analyzer binary executable

# Time interval in milliseconds between perf_analyzer measurements.
# perf_analyzer will take measurements over all the requests completed within
# this time interval.
perf_measurement_window: 15000

# Stops writing the output from the perf_analyzer to stdout.

# Triton Server version used when launching using Docker mode

# Logging level

# Triton Server HTTP endpoint url used by Model Analyzer client. Will be ignored if server-launch-mode is not 'remote'".

# Triton Server GRPC endpoint url used by Model Analyzer client. Will be ignored if server-launch-mode is not 'remote'".

# Triton Server metrics endpoint url used by Model Analyzer client. Will be ignored if server-launch-mode is not 'remote'".

# The full path to the tritonserver binary executable

# The full path to a file to write the Triton Server output log.

# List of GPU UUIDs to be used for the profiling. Use 'all' to profile all the GPUs visible by CUDA."
gpus: 'all'

# Constraints to filter out the results
constraints:
    perf_latency:
      max: 10

# Using manual config search
run_config_search_disable: True

# Save intermediate model repositories
override_output_model_repository: True
output_model_repository_path: "/home/scratch.asardana_sw/tfs/DeepLearningExamples/TensorFlow/Recommendation/WideAndDeep/results/model_analyzer/output_model/my_model"

# Prevent perf_analyzer from getting killed:
perf_analyzer_cpu_util: 300

# Number of top configs to capture in the summary report
top_n_configs: 10 